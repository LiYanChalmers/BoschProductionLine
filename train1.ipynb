{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "- Input:\n",
    "    - Numerical data with important features\n",
    "    - Engineered time features\n",
    "- Output:\n",
    "    - submission\n",
    "- Model:\n",
    "    - XGBoost\n",
    "    \n",
    "Time features:\n",
    "1. Together with test data\n",
    "    1. ID difference to the previous and next row when sorted by date_start and ID, Time_analysis.ipynb.\n",
    "    2. ID difference to the previous and next row when sorted by date_end and ID, Time_analysis.ipynb.\n",
    "    3. ID difference to the previous and next row when sorted by line start time and ID, Time_analysis.ipynb.\n",
    "    \n",
    "2. Independent of test data\n",
    "    4. Duration of whole production binned by 8 or 12 hours, e.g., Time_analysis.ipynb.\n",
    "    5. The binned day of a week, hour of a week, and hour of a day for date_start and date_end, e.g., Time_analysis.ipynb\n",
    "    6. Binned date_start, date_end, and duration on each station and line, e.g., station_time.ipynb and line_time.ipynb. \n",
    "        - **The bin edges should be saved for test data.**\n",
    "    7. Station flows converted to a number. \n",
    "        - **This is actually a categorical feature, should make XGBoost tree deep.**\n",
    "    8. Segments of production duration\n",
    "    \n",
    "3. Out of fold features\n",
    "    9. Row distance to the previous error when sorted by ID\n",
    "    10. Row distance to the previous error when sorted by date_start and ID\n",
    "    11. Row distance to the previous error when sorted by date_end and ID\n",
    "    12. The bayesian mean of (E) \n",
    "    13. The bayesian mean of (F) \n",
    "    14. The bayesian mean of (G)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sys import getsizeof\n",
    "import time\n",
    "import gc\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(x, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(x, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_pickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        x = pickle.load(handle)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time features: Together with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with test data\n",
    "\n",
    "- ID difference to the previous and next row when sorted by date_start and ID, Time_analysis.ipynb.\n",
    "- ID difference to the previous and next row when sorted by date_end and ID, Time_analysis.ipynb.\n",
    "- ID difference to the previous and next row when sorted by line start time and ID, Time_analysis.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_station_time(df, bin_edges=None):\n",
    "    '''\n",
    "    Calculate start time, end time, and duration for parts on each station\n",
    "    \n",
    "    Output:\n",
    "    station_time: the start, end, and duration time on each station\n",
    "    station_time_binned: the binned data\n",
    "    station_time_bins: contains bin edges to transform new data\n",
    "    '''\n",
    "    \n",
    "    # list of station names\n",
    "    stations = list(set([f.split('_')[1] for f in df.columns.tolist() if f!='Response']))\n",
    "    # features in each station\n",
    "    station_features = {s: [f for f in df.columns.tolist() if s in f] for s in stations }\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    # samples in each feature\n",
    "    feature_samples = {}\n",
    "    print('Calculating feature samples:')\n",
    "    for col in tqdm.tqdm_notebook(columns):\n",
    "        feature_samples[col] = len(df.loc[df[col].notna(), col])    \n",
    "    feature_samples = pd.Series(feature_samples)\n",
    "    feature_samples.sort_values(ascending=False, inplace=True)\n",
    "    feature_samples = feature_samples.reset_index()\n",
    "    feature_samples.columns = ['feature', 'count']\n",
    "    feature_samples['station'] = feature_samples['feature'].apply(lambda x: x.split('_')[1])\n",
    "    \n",
    "    # samples per station\n",
    "    station_samples = feature_samples.groupby('station')['count'].max().sort_values(ascending=False)\n",
    "    \n",
    "    # start and end times and durations for each part on each station\n",
    "    station_time = {}\n",
    "    for s in tqdm.tqdm_notebook(stations):\n",
    "        station_time[s+'_start'] = df[station_features[s]].min(axis=1)\n",
    "        station_time[s+'_end'] = df[station_features[s]].max(axis=1)\n",
    "        station_time[s+'_duration'] = station_time[s+'_end'] - station_time[s+'_start']\n",
    "    \n",
    "    # Read the useful columns, discard the rest\n",
    "    station_columns = read_pickle('station_time_columns.pickle')\n",
    "    station_time2 = {}\n",
    "    for k in station_columns:\n",
    "        station_time2[k] = station_time[k]\n",
    "    station_time = station_time2\n",
    "    del station_time2\n",
    "    station_time = pd.DataFrame(station_time)\n",
    "    \n",
    "    # stores bin edges and labels for the categorical version of station_time\n",
    "    station_time_bins = {}\n",
    "    station_time_binned = station_time.copy()\n",
    "    for f in tqdm.tqdm_notebook(station_columns):\n",
    "        if not bin_edges:\n",
    "            # if bins are not provided, use quantile cut\n",
    "            bins = int(max(10, station_samples[f.split('_')[0]]/20000))\n",
    "            station_time_binned[f], station_time_bins[f] = pd.qcut(station_time[f], retbins=True,\n",
    "                q=bins, labels=False, duplicates='drop')\n",
    "        else:\n",
    "            # if bin edges are provided, use cut\n",
    "            station_time_binned[f], station_time_bins[f] = pd.cut(station_time[f], retbins=True,\n",
    "                bins=bin_edges[f], labels=False, duplicates='drop')\n",
    "    \n",
    "    return station_time, station_time_binned, station_time_bins\n",
    "\n",
    "# Test\n",
    "#df = pd.read_csv('data/train_date.csv.zip', index_col=0, nrows=1000)\n",
    "#station_time, station_time_binned, station_time_bins = calculate_station_time(df)\n",
    "#df = pd.read_csv('data/test_date.csv.zip', index_col=0, nrows=1000)\n",
    "#station_time, station_time_binned, station_time_bins = calculate_station_time(df, bin_edges=station_time_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/li/miniconda3/envs/kaggle/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of x_date is 2.746G.\n",
      "Memory usage of x_date is 2.746G.\n"
     ]
    }
   ],
   "source": [
    "# Load date data\n",
    "x_train_date = pd.read_csv('data/train_date.csv.zip', index_col=0, dtype=np.float16)\n",
    "x_test_date = pd.read_csv('data/test_date.csv.zip', index_col=0, dtype=np.float16)\n",
    "print('Memory usage of x_date is {:.3f}G.'.format(x_train_date.memory_usage(deep=True).sum()*10**-9))\n",
    "print('Memory usage of x_date is {:.3f}G.'.format(x_test_date.memory_usage(deep=True).sum()*10**-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test data together\n",
    "x_all_date = pd.concat([x_train_date, x_test_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start and end time of parts\n",
    "# first create a series, then add it to the dataframe, this is more memory efficient\n",
    "date_start = x_all_date.min(axis=1).values\n",
    "x_all_date['date_start'] = date_start\n",
    "date_end = x_all_date.max(axis=1).values\n",
    "x_all_date['date_end'] = date_end\n",
    "\n",
    "del date_start, date_end\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID distance to the previous and next row when sorted by ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0_S0_D1</th>\n",
       "      <th>L0_S0_D3</th>\n",
       "      <th>L0_S0_D5</th>\n",
       "      <th>L0_S0_D7</th>\n",
       "      <th>L0_S0_D9</th>\n",
       "      <th>L0_S0_D11</th>\n",
       "      <th>L0_S0_D13</th>\n",
       "      <th>L0_S0_D15</th>\n",
       "      <th>L0_S0_D17</th>\n",
       "      <th>L0_S0_D19</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_S50_D4250</th>\n",
       "      <th>L3_S50_D4252</th>\n",
       "      <th>L3_S50_D4254</th>\n",
       "      <th>L3_S51_D4255</th>\n",
       "      <th>L3_S51_D4257</th>\n",
       "      <th>L3_S51_D4259</th>\n",
       "      <th>L3_S51_D4261</th>\n",
       "      <th>L3_S51_D4263</th>\n",
       "      <th>date_start</th>\n",
       "      <th>date_end</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.25</td>\n",
       "      <td>87.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1313.00</td>\n",
       "      <td>1316.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1624.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1154.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>602.50</td>\n",
       "      <td>606.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      L0_S0_D1  L0_S0_D3  L0_S0_D5  L0_S0_D7  L0_S0_D9  L0_S0_D11  L0_S0_D13  \\\n",
       "Id                                                                             \n",
       "4.0      82.25     82.25     82.25     82.25     82.25      82.25      82.25   \n",
       "6.0        NaN       NaN       NaN       NaN       NaN        NaN        NaN   \n",
       "7.0    1619.00   1619.00   1619.00   1619.00   1619.00    1619.00    1619.00   \n",
       "9.0    1149.00   1149.00   1149.00   1149.00   1149.00    1149.00    1149.00   \n",
       "11.0    602.50    602.50    602.50    602.50    602.50     602.50     602.50   \n",
       "\n",
       "      L0_S0_D15  L0_S0_D17  L0_S0_D19    ...      L3_S50_D4250  L3_S50_D4252  \\\n",
       "Id                                       ...                                   \n",
       "4.0       82.25      82.25      82.25    ...               NaN           NaN   \n",
       "6.0         NaN        NaN        NaN    ...               NaN           NaN   \n",
       "7.0     1619.00    1619.00    1619.00    ...               NaN           NaN   \n",
       "9.0     1149.00    1149.00    1149.00    ...               NaN           NaN   \n",
       "11.0     602.50     602.50     602.50    ...               NaN           NaN   \n",
       "\n",
       "      L3_S50_D4254  L3_S51_D4255  L3_S51_D4257  L3_S51_D4259  L3_S51_D4261  \\\n",
       "Id                                                                           \n",
       "4.0            NaN           NaN           NaN           NaN           NaN   \n",
       "6.0            NaN           NaN           NaN           NaN           NaN   \n",
       "7.0            NaN           NaN           NaN           NaN           NaN   \n",
       "9.0            NaN           NaN           NaN           NaN           NaN   \n",
       "11.0           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "      L3_S51_D4263  date_start   date_end  \n",
       "Id                                         \n",
       "4.0            NaN       82.25    87.3125  \n",
       "6.0            NaN     1313.00  1316.0000  \n",
       "7.0            NaN     1619.00  1624.0000  \n",
       "9.0            NaN     1149.00  1154.0000  \n",
       "11.0           NaN      602.50   606.0000  \n",
       "\n",
       "[5 rows x 1158 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ID information\n",
    "x_all_date.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID difference for only train set when sorted by ID\n",
    "diff1 = x_all_date['Id'].diff().fillna(9999999)\n",
    "x_all_date['diff1'] = x_all_date['Id'].diff().fillna(9999999)\n",
    "diff2 = x_all_date['Id'].diff(-1).fillna(9999999)\n",
    "x_all_date['diff2'] = diff2\n",
    "\n",
    "# ID difference for both train and test when sorted by time and ID\n",
    "x_all_date.sort_values(by=['date_start', 'Id'], inplace=True)\n",
    "diff3 = x_all_date['Id'].diff().fillna(9999999)\n",
    "x_all_date['diff3'] = diff3\n",
    "diff4 = x_all_date['Id'].diff(-1).fillna(9999999)\n",
    "x_all_date['diff4'] = diff4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load numerical data\n",
    "x_train_num = pd.read_csv('data/train_numeric.csv.zip', index_col=0, dtype=np.float16)\n",
    "y_train = x_train_num['Response']\n",
    "y_train = y_train.astype(int)\n",
    "x_train_num.drop('Response', axis=1, inplace=True)\n",
    "\n",
    "n_train, n_num = x_train_num.shape\n",
    "\n",
    "# Sample numerical data, obtain feature importance\n",
    "idx = np.random.randint(0, n_train, 200000)\n",
    "\n",
    "x_sample = x_train_num.iloc[idx].values\n",
    "y_sample = y_train.iloc[idx].values\n",
    "\n",
    "# Train XGBoost\n",
    "clf = XGBClassifier(base_score=0.0058, max_depth=6, n_jobs=6)\n",
    "clf.fit(x_sample, y_sample, verbose=True)\n",
    "\n",
    "# Plot feature importance\n",
    "important_indices = np.where(clf.feature_importances_>0.001)[0]\n",
    "plt.plot(sorted(clf.feature_importances_))\n",
    "plt.plot(0.001*np.ones(len(clf.feature_importances_)))\n",
    "\n",
    "# We have selected 242 important features\n",
    "important_numerical_features = x_train_num.columns[important_indices]\n",
    "print(important_numerical_features)\n",
    "print(len(important_numerical_features))\n",
    "\n",
    "# Save names of the important features.\n",
    "important_numerical_features = pd.DataFrame(important_numerical_features)\n",
    "important_numerical_features.to_csv('important_numerical_features_samples_2e5.csv')\n",
    "\n",
    "# Remove other features for the time being to save memory.\n",
    "x_train_num.drop([c for c in x_train_num.columns.values if c not in important_numerical_features.values.reshape(-1,)], \n",
    "                axis=1, inplace=True)\n",
    "\n",
    "print('Memory usage of x_num is {:.3f}G.'.format(x_train_num.memory_usage(deep=True).sum()*10**-9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
