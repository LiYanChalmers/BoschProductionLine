{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "- Input:\n",
    "    - Numerical data with important features\n",
    "    - Engineered time features\n",
    "- Output:\n",
    "    - submission\n",
    "- Model:\n",
    "    - XGBoost\n",
    "    \n",
    "Time features:\n",
    "1. Together with test data\n",
    "    1. ID difference to the previous and next row when sorted by date_start and ID, Time_analysis.ipynb.\n",
    "    2. ID difference to the previous and next row when sorted by date_end and ID, Time_analysis.ipynb.\n",
    "    3. ID difference to the previous and next row when sorted by line start time and ID, Time_analysis.ipynb.\n",
    "    \n",
    "2. Independent of test data\n",
    "    4. Duration of whole production binned by 8 or 12 hours, e.g., Time_analysis.ipynb.\n",
    "    5. The binned day of a week, hour of a week, and hour of a day for date_start and date_end, e.g., Time_analysis.ipynb\n",
    "    6. Binned date_start, date_end, and duration on each station and line, e.g., station_time.ipynb and line_time.ipynb. \n",
    "        - **The bin edges should be saved for test data.**\n",
    "    7. Station flows converted to a number. \n",
    "        - **This is actually a categorical feature, should make XGBoost tree deep.**\n",
    "    8. Segments of production duration\n",
    "    \n",
    "3. Out of fold features\n",
    "    9. Row distance to the previous error when sorted by ID\n",
    "    10. Row distance to the previous error when sorted by date_start and ID\n",
    "    11. Row distance to the previous error when sorted by date_end and ID\n",
    "    12. The bayesian mean of (E) \n",
    "    13. The bayesian mean of (F) \n",
    "    14. The bayesian mean of (G)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sys import getsizeof\n",
    "import time\n",
    "import gc\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(x, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(x, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_pickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        x = pickle.load(handle)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\numpy\\lib\\arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of x_train_date is 2.746G.\n",
      "Memory usage of x_test_date is 2.746G.\n"
     ]
    }
   ],
   "source": [
    "# columns in date data\n",
    "col_types = pd.read_csv('data/train_date.csv.zip', nrows=1)\n",
    "col_types = pd.read_csv('data/train_date.csv.zip', nrows=1)\n",
    "col_types = {k: np.float16 for k in col_types.columns}\n",
    "col_types['Id'] = np.int64\n",
    "\n",
    "# Load date data\n",
    "x_train_date = pd.read_csv('data/train_date.csv.zip', index_col=0, dtype=col_types, engine='c')\n",
    "x_test_date = pd.read_csv('data/test_date.csv.zip', index_col=0, dtype=col_types, engine='c')\n",
    "print('Memory usage of x_train_date is {:.3f}G.'.format(x_train_date.memory_usage(deep=True).sum()*10**-9))\n",
    "print('Memory usage of x_test_date is {:.3f}G.'.format(x_test_date.memory_usage(deep=True).sum()*10**-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>Id</th>\n",
       "      <th>L0_S0_D1</th>\n",
       "      <th>L0_S0_D3</th>\n",
       "      <th>L0_S0_D5</th>\n",
       "      <th>L0_S0_D7</th>\n",
       "      <th>L0_S0_D9</th>\n",
       "      <th>L0_S0_D11</th>\n",
       "      <th>L0_S0_D13</th>\n",
       "      <th>L0_S0_D15</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_S50_D4246</th>\n",
       "      <th>L3_S50_D4248</th>\n",
       "      <th>L3_S50_D4250</th>\n",
       "      <th>L3_S50_D4252</th>\n",
       "      <th>L3_S50_D4254</th>\n",
       "      <th>L3_S51_D4255</th>\n",
       "      <th>L3_S51_D4257</th>\n",
       "      <th>L3_S51_D4259</th>\n",
       "      <th>L3_S51_D4261</th>\n",
       "      <th>L3_S51_D4263</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>82.25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>1619.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>1149.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>602.50</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx  Id  L0_S0_D1  L0_S0_D3  L0_S0_D5  L0_S0_D7  L0_S0_D9  L0_S0_D11  \\\n",
       "Id                                                                         \n",
       "4     0   4     82.25     82.25     82.25     82.25     82.25      82.25   \n",
       "6     1   6       NaN       NaN       NaN       NaN       NaN        NaN   \n",
       "7     2   7   1619.00   1619.00   1619.00   1619.00   1619.00    1619.00   \n",
       "9     3   9   1149.00   1149.00   1149.00   1149.00   1149.00    1149.00   \n",
       "11    4  11    602.50    602.50    602.50    602.50    602.50     602.50   \n",
       "\n",
       "    L0_S0_D13  L0_S0_D15      ...       L3_S50_D4246  L3_S50_D4248  \\\n",
       "Id                            ...                                    \n",
       "4       82.25      82.25      ...                NaN           NaN   \n",
       "6         NaN        NaN      ...                NaN           NaN   \n",
       "7     1619.00    1619.00      ...                NaN           NaN   \n",
       "9     1149.00    1149.00      ...                NaN           NaN   \n",
       "11     602.50     602.50      ...                NaN           NaN   \n",
       "\n",
       "    L3_S50_D4250  L3_S50_D4252  L3_S50_D4254  L3_S51_D4255  L3_S51_D4257  \\\n",
       "Id                                                                         \n",
       "4            NaN           NaN           NaN           NaN           NaN   \n",
       "6            NaN           NaN           NaN           NaN           NaN   \n",
       "7            NaN           NaN           NaN           NaN           NaN   \n",
       "9            NaN           NaN           NaN           NaN           NaN   \n",
       "11           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "    L3_S51_D4259  L3_S51_D4261  L3_S51_D4263  \n",
       "Id                                            \n",
       "4            NaN           NaN           NaN  \n",
       "6            NaN           NaN           NaN  \n",
       "7            NaN           NaN           NaN  \n",
       "9            NaN           NaN           NaN  \n",
       "11           NaN           NaN           NaN  \n",
       "\n",
       "[5 rows x 1158 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = x_train_date.shape[0]\n",
    "x_all_date = pd.concat([x_train_date, x_test_date])\n",
    "#del x_train_date, x_test_date\n",
    "#gc.collect()\n",
    "\n",
    "x_all_date.reset_index(inplace=True)\n",
    "x_all_date.reset_index(inplace=True)\n",
    "x_all_date.set_index('Id', drop=False, inplace=True)\n",
    "x_all_date.rename(columns={'index':'idx'}, inplace=True)\n",
    "x_all_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with test data\n",
    "\n",
    "- Start/end times when part flows through a station, continuous and binned.\n",
    "- Start/end times when part flows through a line, continuous and binned.\n",
    "- ID difference when rows are sorted by certain creteria\n",
    "    - Start/end times flowing through station, line, or overall-start/end times\n",
    "    - ID\n",
    "- Modulo time in a period\n",
    "    - 2.5h, 24h, or 168h\n",
    "- Number of samples in the same time (6 mins)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_station_time(df, bin_edges=None):\n",
    "    '''\n",
    "    Calculate start time, end time, and duration for parts on each station\n",
    "    \n",
    "    Output:\n",
    "    station_time: the start, end, and duration time on each station\n",
    "    station_time_binned: the binned data\n",
    "    station_time_bins: contains bin edges to transform new data\n",
    "    '''\n",
    "    \n",
    "    columns = [f for f in df.columns.tolist() if f not in ['Response', 'Id', 'idx'] ]\n",
    "    # list of station names\n",
    "    stations = list(set([f.split('_')[1] for f in columns]))\n",
    "    # features in each station\n",
    "    station_features = {s: [f for f in columns if s in f] for s in stations }\n",
    "    \n",
    "    # samples in each feature\n",
    "    feature_samples = {}\n",
    "    print('Calculating feature samples:')\n",
    "    for col in tqdm.tqdm_notebook(columns):\n",
    "        feature_samples[col] = len(df.loc[df[col].notna(), col])    \n",
    "    feature_samples = pd.Series(feature_samples)\n",
    "    feature_samples.sort_values(ascending=False, inplace=True)\n",
    "    feature_samples = feature_samples.reset_index()\n",
    "    feature_samples.columns = ['feature', 'count']\n",
    "    feature_samples['station'] = feature_samples['feature'].apply(lambda x: x.split('_')[1])\n",
    "    \n",
    "    # samples per station\n",
    "    station_samples = feature_samples.groupby('station')['count'].max().sort_values(ascending=False)\n",
    "    \n",
    "    # start and end times and durations for each part on each station\n",
    "    station_time = {}\n",
    "    print('Calculate station times:')\n",
    "    for s in tqdm.tqdm_notebook(stations):\n",
    "        station_time[s+'_start'] = df[station_features[s]].min(axis=1)\n",
    "        station_time[s+'_end'] = df[station_features[s]].max(axis=1)\n",
    "        station_time[s+'_duration'] = station_time[s+'_end'] - station_time[s+'_start']\n",
    "    \n",
    "    # Read the useful columns, discard the rest\n",
    "    station_columns = read_pickle('station_time_columns.pickle')\n",
    "    station_time2 = {}\n",
    "    for k in station_columns:\n",
    "        station_time2[k] = station_time[k]\n",
    "    station_time = station_time2\n",
    "    del station_time2\n",
    "    station_time = pd.DataFrame(station_time)\n",
    "    \n",
    "    # stores bin edges and labels for the categorical version of station_time\n",
    "    station_time_bins = {}\n",
    "    station_time_binned = {}\n",
    "    print('Calculate binned station times:')\n",
    "    for f in tqdm.tqdm_notebook(station_columns):\n",
    "        if not bin_edges:\n",
    "            # if bins are not provided, use quantile cut\n",
    "            bins = int(max(10, station_samples[f.split('_')[0]]/20000))\n",
    "            station_time_binned[f+'_bin'], station_time_bins[f] = pd.qcut(station_time[f], retbins=True,\n",
    "                q=bins, labels=False, duplicates='drop')\n",
    "        else:\n",
    "            # if bin edges are provided, use cut\n",
    "            station_time_binned[f+'_bin'], station_time_bins[f] = pd.cut(station_time[f], retbins=True,\n",
    "                bins=bin_edges[f], labels=False, duplicates='drop')\n",
    "            \n",
    "    station_time_binned = pd.DataFrame(station_time_binned)\n",
    "    \n",
    "    return station_time, station_time_binned, station_time_bins\n",
    "\n",
    "# Test\n",
    "#df = pd.read_csv('data/train_date.csv.zip', index_col=0, nrows=1000)\n",
    "#station_time, station_time_binned, station_time_bins = calculate_station_time(df)\n",
    "#df = pd.read_csv('data/test_date.csv.zip', index_col=0, nrows=1000)\n",
    "#station_time, station_time_binned, station_time_bins = calculate_station_time(df, bin_edges=station_time_bins)\n",
    "#station_time, station_time_binned, _ = calculate_station_time(x_all_date, bin_edges=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_line_time(df, bin_edges=None):\n",
    "    '''\n",
    "    Calculate start, end, and duration time for each part on each line.\n",
    "    Then calculate ID difference (+/- 1) when sorted by line start and end time\n",
    "    '''\n",
    "    \n",
    "    # features in each line\n",
    "    columns = [f for f in df.columns.tolist() if f not in ['Response', 'Id', 'idx'] ]\n",
    "    line_features = {k: [c for c in columns if c.split('_')[0]==k] for k in ['L0', 'L1', 'L2', 'L3']}\n",
    "    \n",
    "    # start and end times for each part in each line\n",
    "    N = 1000 # calculate N rows each time, save RAM\n",
    "    line = {} # contains results\n",
    "    n_parts = df.shape[0]\n",
    "    for l in line_features.keys():\n",
    "        line[l+'_min'] = []\n",
    "        line[l+'_max'] = []\n",
    "        print('Calculating start and end times for line {}'.format(l))\n",
    "        for i in tqdm.tqdm_notebook(range(n_parts//N+1)):\n",
    "            tmp = df.iloc[i*N:min((i+1)*N, n_parts)].copy()\n",
    "            line[l+'_min'].append(tmp[line_features[l]].min(axis=1))\n",
    "            line[l+'_max'].append(tmp[line_features[l]].max(axis=1))\n",
    "        line[l+'_min'] = pd.concat(line[l+'_min'])\n",
    "        line[l+'_max'] = pd.concat(line[l+'_max'])\n",
    "    # convert results to dataframe\n",
    "    line = pd.DataFrame(line)\n",
    "    \n",
    "    # convert results to binned categorical data\n",
    "    line_binned = {}\n",
    "    line_bins = {}\n",
    "    print('Calculate binned line times:')\n",
    "    for f in tqdm.tqdm_notebook(line.columns):\n",
    "        if not bin_edges:\n",
    "            # if bin edges are not provided, use quantile cuts\n",
    "            bins = int(max(10, line['L0_min'].notna().sum()/20000))\n",
    "            line_binned[f+'_bin'], line_bins[f] = pd.qcut(line[f], bins, labels=False, \n",
    "                retbins=True, duplicates='drop')\n",
    "        else:\n",
    "            # use given bin edges\n",
    "            line_binned[f+'_bin'], line_bins[f] = pd.cut(line[f], bin_edges[f], labels=False, \n",
    "                retbins=True, duplicates='drop')\n",
    "        \n",
    "    line_binned = pd.DataFrame(line_binned)\n",
    "    \n",
    "    return line, line_binned, line_bins\n",
    "\n",
    "#line, line_binned, line_bins = calculate_line_time(x_all_date, bin_edges=None)\n",
    "#line, line_binned, line_bins = calculate_line_time(x_all_date, bin_edges=line_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_id_diff(df, feats_sort, feats_new):\n",
    "    '''\n",
    "    Calculate ID difference between previous and next row when sorted by some creteria.\n",
    "    \n",
    "    Input:\n",
    "    df: a copy of the original dataframe with only relevant columns. df should have a 'Id' column\n",
    "    feats_sort: columns df is sorted by\n",
    "    feats_new: a list containing two names for the new features\n",
    "    Output:\n",
    "    df with two new columns\n",
    "    '''\n",
    "    df.sort_values(feats_sort, inplace=True)\n",
    "    df[feats_new[0]] = df.Id.diff().fillna(9999999)\n",
    "    df[feats_new[1]] = df.Id.diff(-1).fillna(9999999)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_modulo(df, suffix='_mod', period=16.8):\n",
    "    '''\n",
    "    Time of the period, e.g., week=16.8, day=2.4\n",
    "    df is a copy of original dataframe, it has Id as index and only relevent columns.\n",
    "    '''\n",
    "    dt = df.copy()\n",
    "    cols = dt.columns.tolist()\n",
    "    for c in cols:\n",
    "        dt[c+suffix] = dt[c] % period\n",
    "    dt.drop(cols, axis=1, inplace=True)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_count(df, feats):\n",
    "    '''\n",
    "    Calculate sample counts for each value of feat in df\n",
    "    '''\n",
    "    dt = df.copy()\n",
    "    for f in feats:\n",
    "        tmp = dt.groupby(f)[f].count()\n",
    "        dt[f+'_count'] = dt[f].map(tmp)\n",
    "    dt.drop(feats, axis=1, inplace=True)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_count_shift(df, suffix, period=16.8, shift=1):\n",
    "    '''\n",
    "    Calculate the number of samples in the shifted (next or previous) period of time bin (2.5h, 24h, or 168h).\n",
    "    shift=1 for next, shift=-1 for previous, shift=0 for no shift\n",
    "    '''\n",
    "    dt = df.copy()\n",
    "    cols = dt.columns.tolist()\n",
    "    for f in cols:\n",
    "        tmp_count = (dt[f] / period) // 1.0\n",
    "        tmp_group = tmp_count.groupby(tmp_count).count()\n",
    "        tmp_group = tmp_group.shift(-shift).fillna(0)\n",
    "        dt[f+suffix] = tmp_count.map(tmp_group)\n",
    "    dt.drop(cols, axis=1, inplace=True)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating start and end times for line L0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30e2301e95744e5ad76b55124d09938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2368), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating start and end times for line L1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5562e65bf87a41d3ac2ba5df98007652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2368), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating start and end times for line L2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20212a18e1ee4203887f21edc06839c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2368), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating start and end times for line L3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc39f55b409444bd8dc473146f0ecb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2368), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculate binned line times:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f3a710e96a4a5d89b508802a72c3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating feature samples:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806208f2939c4108928cd4a907d069bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculate station times:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faca2500f57416d9bdd7a62e451feb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process line and station times\n",
    "line, line_binned, _ = calculate_line_time(x_all_date, bin_edges=None)\n",
    "station, station_binned, _ = calculate_station_time(x_all_date, bin_edges=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ID differences for line times\n",
    "# according to previous data analysis (line_time.ipynb), L1 and L2 are not useful here\n",
    "cols = ['L0_min', 'L0_max', 'L3_min', 'L3_max'] \n",
    "line_diff_id = line[cols].copy()\n",
    "line_diff_id.reset_index(inplace=True)\n",
    "\n",
    "for c in tqdm.tqdm_notebook(cols):\n",
    "    line_diff_id = calculate_id_diff(line_diff_id, [c, 'Id'], [c+'_diff1', c+'_diff2'])\n",
    "line_diff_id.drop(cols, axis=1, inplace=True)\n",
    "line_diff_id.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ID differences for station times\n",
    "cols = station.columns.tolist()\n",
    "station_diff_id = station.copy()\n",
    "station_diff_id.reset_index(inplace=True)\n",
    "\n",
    "for c in tqdm.tqdm_notebook(cols):\n",
    "    station_diff_id = calculate_id_diff(station_diff_id, [c, 'Id'], [c+'_diff1', c+'_diff2'])\n",
    "station_diff_id.drop(cols, axis=1, inplace=True)\n",
    "station_diff_id.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID difference when sorted by start and end of whole data set\n",
    "time_diff_id = line.copy()\n",
    "cols = time_diff_id.columns.tolist()\n",
    "\n",
    "# calculate min and max times\n",
    "time_diff_id['time_min'] = time_diff_id.min(axis=1)\n",
    "time_diff_id['time_max'] = time_diff_id.max(axis=1)\n",
    "time_diff_id.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "# add idx\n",
    "idx = x_all_date['idx'].copy()\n",
    "time_diff_id = time_diff_id.join(idx)\n",
    "time_diff_id.sort_values('idx', inplace=True)\n",
    "time_diff_id.reset_index(inplace=True)\n",
    "\n",
    "# ID difference when sorted by time\n",
    "cols = ['time_min', 'time_max']\n",
    "for c in tqdm.tqdm_notebook(cols):\n",
    "    time_diff_id = calculate_id_diff(time_diff_id, [c, 'Id'], [c+'_diff1', c+'_diff2'])\n",
    "# ID difference when sorted by index\n",
    "time_diff_id = calculate_id_diff(time_diff_id, ['idx'], ['idx_diff1', 'idx_diff2'])\n",
    "\n",
    "# duration of total time\n",
    "time_diff_id['time_duration'] = time_diff_id['time_max'] - time_diff_id['time_min']\n",
    "\n",
    "# remove index\n",
    "time_diff_id.drop('idx', axis=1, inplace=True)\n",
    "# set ID as index\n",
    "time_diff_id.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour of the day/week\n",
    "time_all = time_diff_id[['time_min', 'time_max']].copy()\n",
    "time_in_qd = time_modulo(time_all, suffix='_mod_qd', period=0.25)\n",
    "time_in_day = time_modulo(time_all, suffix='_mod_day', period=2.4)\n",
    "time_in_week = time_modulo(time_all, suffix='_mod_week', period=16.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_counts = calculate_count(time_all, ['time_min', 'time_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_count_shift_p1w = calculate_count_shift(time_all, '_p1w', 16.8, 1) # p1w for plus 1 week\n",
    "time_count_shift_m1w = calculate_count_shift(time_all, '_m1w', 16.8, -1) # m1w for minus 1 week\n",
    "\n",
    "time_count_shift_p1d = calculate_count_shift(time_all, '_p1d', 2.4, 1) # p1w for plus 1 day\n",
    "time_count_shift_m1d = calculate_count_shift(time_all, '_m1d', 2.4, -1) # m1w for minus 1 day\n",
    "\n",
    "time_count_shift_p1qd = calculate_count_shift(time_all, '_p1qd', 0.25, 1) # p1w for plus 1 quarter day\n",
    "time_count_shift_m1qd = calculate_count_shift(time_all, '_m1qd', 0.25, -1) # m1w for minus 1 quarter day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_feats = [line, line_binned, line_diff_id, \n",
    "              station, station_binned, station_diff_id, \n",
    "              time_in_day, time_in_week, time_in_qd,\n",
    "              time_counts,\n",
    "              time_count_shift_p1w, time_count_shift_m1w,\n",
    "              time_count_shift_p1d, time_count_shift_m1d,\n",
    "              time_count_shift_p1qd, time_count_shift_m1qd, idx]\n",
    "x_all_date_feats = time_diff_id.join(time_feats)\n",
    "x_all_date_feats['na_counts'] = x_all_date_feats.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(idx.index, idx.values) # this means that when sorted by idx values, the first n_train samples are train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_date_feats.sort_values('idx', inplace=True)\n",
    "x_train_date_feats = np.float32(x_all_date_feats.iloc[:n_train].values)\n",
    "x_test_date_feats = np.float32(x_all_date_feats.iloc[n_train:].values)\n",
    "print('Train data: ', x_train_date_feats.shape)\n",
    "print('Test data: ', x_test_date_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(x_train_date_feats, 'x_train_date_feats_0.pickle')\n",
    "save_pickle(x_test_date_feats, 'x_test_date_feats_0.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian features\n",
    "\n",
    "- Mean time diff to last/next 1/5/10 failures\n",
    "- row distance to last/next 1/5/10 failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('data/train_numeric.csv.zip', index_col=0, usecols=[0, 969])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_time = time_all.join(idx)\n",
    "#x_train_time.rename(columns={'idx': 'idx_order'}, inplace=True)\n",
    "x_train_time.sort_values('idx', axis=0, inplace=True)\n",
    "u = x_train_time.iloc[:n_train]\n",
    "v = x_train_time.iloc[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.drop('dist_1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = x.sort_values(['Response', 'time_min'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = xs.loc[xs.Response==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.loc[xs.time_min<t].iloc[-5:]['time_min'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['bayesian'] = x['time_min'].apply(lambda x: xs.loc[xs['time_min']>x].iloc[:5]['time_min'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
