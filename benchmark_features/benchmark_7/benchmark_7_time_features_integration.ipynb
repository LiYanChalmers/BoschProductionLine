{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for measuring performance of engineered features and models\n",
    "Add features created from previous and next records in original order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/c3se/NOBACKUP/users/lyaa/conda_dir/miniconda/envs/kaggle/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.name=='nt':\n",
    "    try:\n",
    "        mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-8.1.0-posix-seh-rt_v6-rev0\\\\mingw64\\\\bin'\n",
    "        os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sys import getsizeof\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from scipy import fftpack\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(x, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(x, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_pickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        x = pickle.load(handle)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def mcc(tp, tn, fp, fn):\n",
    "    sup = tp * tn - fp * fn\n",
    "    inf = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    if inf==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sup / np.sqrt(inf)\n",
    "\n",
    "@jit\n",
    "def eval_mcc(y_true, y_prob, show=False):\n",
    "    idx = np.argsort(y_prob)\n",
    "    y_true_sort = y_true[idx]\n",
    "    n = y_true.shape[0]\n",
    "    nump = 1.0 * np.sum(y_true) # number of positive\n",
    "    numn = n - nump # number of negative\n",
    "    tp = nump\n",
    "    tn = 0.0\n",
    "    fp = numn\n",
    "    fn = 0.0\n",
    "    best_mcc = 0.0\n",
    "    best_id = -1\n",
    "    prev_proba = -1\n",
    "    best_proba = -1\n",
    "    mccs = np.zeros(n)\n",
    "    new_mcc = 0\n",
    "    for i in range(n):\n",
    "        # all items with idx < i are predicted negative while others are predicted positive\n",
    "        # only evaluate mcc when probability changes\n",
    "        proba = y_prob[idx[i]]\n",
    "        if proba != prev_proba:\n",
    "            prev_proba = proba\n",
    "            new_mcc = mcc(tp, tn, fp, fn)\n",
    "            if new_mcc >= best_mcc:\n",
    "                best_mcc = new_mcc\n",
    "                best_id = i\n",
    "                best_proba = proba\n",
    "        mccs[i] = new_mcc\n",
    "        if y_true_sort[i] == 1:\n",
    "            tp -= 1.0\n",
    "            fn += 1.0\n",
    "        else:\n",
    "            fp -= 1.0\n",
    "            tn += 1.0\n",
    "    if show:\n",
    "        y_pred = (y_prob >= best_proba).astype(int)\n",
    "        score = matthews_corrcoef(y_true, y_pred)\n",
    "        print(score, best_mcc)\n",
    "        plt.plot(mccs)\n",
    "        return best_proba, best_mcc, y_pred\n",
    "    else:\n",
    "        return best_mcc\n",
    "    \n",
    "def mcc_eval(y_prob, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    best_mcc = eval_mcc(y_true, y_prob)\n",
    "    return 'MCC', best_mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_predict_rskf(clf, x_train, y_train, n_splits=3, \n",
    "                           n_repeats=2, random_state=42, verbose=False, early_stopping=10):\n",
    "    '''\n",
    "    Repeated stratified KFold CV, returns predictions for \n",
    "    each repeat and average score.\n",
    "    n_repeats: repetitions of CV\n",
    "    to disable erlay stopping, set early_stopping to None\n",
    "    '''\n",
    "    scores = []\n",
    "    n_trees = []\n",
    "    clfs = []\n",
    "    running_time = []\n",
    "    \n",
    "    rskf = RepeatedStratifiedKFold(n_repeats=n_repeats, n_splits=n_splits, \n",
    "                                   random_state=0)\n",
    "    np.random.seed(random_state)\n",
    "    for n, (train_index, test_index) in enumerate(rskf.split(x_train, y_train)):\n",
    "        print('Round {}'.format(n))\n",
    "        start_time = time.time()\n",
    "        x_train_tmp, x_test_tmp = x_train.iloc[train_index], x_train.iloc[test_index]\n",
    "        y_train_tmp, y_test_tmp = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        clf.random_state = np.random.randint(10000000)\n",
    "        \n",
    "        if early_stopping is not None:\n",
    "            clf.fit(x_train_tmp, y_train_tmp, \n",
    "                    eval_set=[(x_test_tmp, y_test_tmp)], \n",
    "                    eval_metric=mcc_eval, early_stopping_rounds=early_stopping,\n",
    "                    verbose=verbose)\n",
    "            scores.append(-clf.best_score)\n",
    "            n_trees.append(clf.best_ntree_limit)\n",
    "        else:\n",
    "            clf.fit(x_train_tmp, y_train_tmp)\n",
    "            scores.append(eval_mcc(y_test_tmp.values, clf.predict_proba(x_test_tmp)[:, 1]))\n",
    "            n_trees.append(clf.n_estimators)\n",
    "        \n",
    "        clfs.append(clf)\n",
    "        running_time.append(time.time()-start_time)\n",
    "        print('Split {}, score = {:.3f}, best_ntree_limit = {}, total time = {:.3f} min'.format(n, scores[n], \n",
    "            n_trees[n], sum(running_time)/60))\n",
    "\n",
    "    print('Score mean = {:.3f}, std = {:.3f}'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return clfs, scores, n_trees, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_predict_skf_rm(clf, x_train, y_train, n_splits=3, \n",
    "                           n_repeats=2, random_state=42, verbose=False, early_stopping=10):\n",
    "    '''\n",
    "    Stratified KFold CV with repeated models\n",
    "    to disable erlay stopping, set early_stopping to None\n",
    "    '''\n",
    "    scores = []\n",
    "    n_trees = []\n",
    "    clfs = []\n",
    "    running_time = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    for m in range(n_repeats):\n",
    "        print('Repeat {}'.format(m))\n",
    "        for n, (train_index, test_index) in enumerate(skf.split(x_train, y_train)):\n",
    "            start_time = time.time()\n",
    "            x_train_tmp, x_test_tmp = x_train.iloc[train_index], x_train.iloc[test_index]\n",
    "            y_train_tmp, y_test_tmp = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            clf.random_state = np.random.randint(10000000)\n",
    "            # print(clf.random_state)\n",
    "\n",
    "            if early_stopping is not None:\n",
    "                clf.fit(x_train_tmp, y_train_tmp, \n",
    "                        eval_set=[(x_test_tmp, y_test_tmp)], \n",
    "                        eval_metric=mcc_eval, early_stopping_rounds=early_stopping,\n",
    "                        verbose=verbose)\n",
    "                scores.append(-clf.best_score)\n",
    "                n_trees.append(clf.best_ntree_limit)\n",
    "            else:\n",
    "                clf.fit(x_train_tmp, y_train_tmp)\n",
    "                scores.append(eval_mcc(y_test_tmp.values, clf.predict_proba(x_test_tmp)[:, 1]))\n",
    "                n_trees.append(clf.n_estimators)\n",
    "            \n",
    "            clfs.append(clf)\n",
    "            running_time.append(time.time() - start_time)\n",
    "            print('Split {}, score = {:.3f}, n_best_trees = {}, total time = {:.3f} min'.format(n, \n",
    "                scores[m*n_repeats+n], n_trees[m*n_repeats+n], sum(running_time)/60))\n",
    "\n",
    "    print('Score mean = {:.3f}, std = {:.3f}'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return clfs, scores, n_trees, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_predict_skf_rm_xgb(params, x_train, y_train, num_boost_round=3, n_splits=3, \n",
    "                           n_repeats=2, random_state=3795264, verbose_eval=False):\n",
    "    '''\n",
    "    Stratified KFold CV with repeated models\n",
    "    Early stopping is totally disabled\n",
    "    Uses xgb.cv API\n",
    "    verbose_eval is the same as in xgb.train\n",
    "    '''\n",
    "    cv_results = {}\n",
    "    clfs = {}\n",
    "    running_time = {}\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=np.random.randint(10**6), shuffle=True)\n",
    "    \n",
    "    for m in range(n_repeats):\n",
    "        for n, (train_index, test_index) in enumerate(skf.split(x_train, y_train)):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Construct DMatrix\n",
    "            dtrain = xgb.DMatrix(x_train.iloc[train_index], label=y_train.iloc[train_index])\n",
    "            dtest = xgb.DMatrix(x_train.iloc[test_index], label=y_train.iloc[test_index])\n",
    "            \n",
    "            # Placeholder for evals_result\n",
    "            cv_results[m, n] = {}\n",
    "            params['seed'] = np.random.randint(10**6)\n",
    "            clfs[m, n] = xgb.train(params, dtrain, num_boost_round=num_boost_round,\n",
    "                                   evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "                                  feval=mcc_eval, maximize=True, early_stopping_rounds=None, \n",
    "                                  evals_result=cv_results[m, n], verbose_eval=verbose_eval)\n",
    "        \n",
    "            running_time[m, n] = time.time() - start_time\n",
    "            \n",
    "            print('Repeat {}, split {}, test MCC = {:.3f}, running time = {:.3f} min'.format(m, n, \n",
    "                cv_results[m, n]['test']['MCC'][-1], running_time[m, n]/60))\n",
    "        \n",
    "    # Post-process cv_results\n",
    "    cv_results_final = {}\n",
    "    for m in range(n_repeats):\n",
    "        for n in range(n_splits):\n",
    "            cv_results_final['train', m, n] = cv_results[m, n]['train']['MCC']\n",
    "            cv_results_final['test', m, n] = cv_results[m, n]['test']['MCC']\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(cv_results_final)\n",
    "    df.index.name = 'iteration'\n",
    "    df.columns.names = ['dataset', 'repeat', 'split']\n",
    "\n",
    "    print('Score mean = {:.3f}, std = {:.3f}'.format(df['test'].iloc[-1].mean(), df['test'].iloc[-1].std()))\n",
    "    \n",
    "    return df, clfs, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_difference_to_failures(x, failure_max):\n",
    "    '''\n",
    "    Find the average time difference to the last and next failure_max failures.\n",
    "    x should have ['Response', 'time_start', 'time_end']\n",
    "    '''\n",
    "    \n",
    "    u = x[['Response', 'time_start']].copy()\n",
    "    u.columns = ['Response', 'time']\n",
    "    x_start = time_difference_to_failures_helper(u, failure_max, '_start')\n",
    "    \n",
    "    u = x[['Response', 'time_end']].copy()\n",
    "    u.columns = ['Response', 'time']\n",
    "    x_end = time_difference_to_failures_helper(u, failure_max, '_end')\n",
    "    \n",
    "    return x_start.join(x_end)\n",
    "\n",
    "def time_difference_to_failures_helper(x, failure_max, suffix):\n",
    "    '''\n",
    "    Find the mean time difference since last/next 1, 2, ..., failure_max failures\n",
    "    when samples are sorted by the time column\n",
    "    suffix is used for column names of the final results\n",
    "    x is a DataFrame containing:\n",
    "    - Both train and test data\n",
    "    - Two columns: Response and a time column, \n",
    "        which is used to sort samples and calculate time differences\n",
    "    '''\n",
    "    \n",
    "    # sort by time and Id\n",
    "    x.sort_values(['time', 'Id'], inplace=True)\n",
    "    x.Response.fillna(0, inplace=True)\n",
    "    x.Response = x.Response.astype(np.int8)\n",
    "    \n",
    "    # ranking in sorted order\n",
    "    x['rank_sort_time'] = np.arange(1, len(x)+1)\n",
    "    \n",
    "    # rank of failures\n",
    "    x['rank_failure'] = x['Response']\n",
    "    x['rank_failure'] = x['rank_failure'].cumsum()\n",
    "    x.loc[x['Response']!=1, 'rank_failure'] = 0\n",
    "    \n",
    "    # the rank_failure of the 1st previous failure for each sample\n",
    "    # for the first several samples without previous failures, use 0\n",
    "    x['fp1'] = x['rank_failure'].shift().fillna(0).astype(np.int64)\n",
    "    x['fp1'] = x['fp1'].cummax()\n",
    "    x['fp1'] = x['fp1'].astype(np.int64)\n",
    "    \n",
    "    # the failure rank of the 2nd to failure_max previous failure for each sample\n",
    "    failure_list = np.arange(2, failure_max+1)\n",
    "    for f in failure_list:\n",
    "        x['fp'+str(f)] = x['fp'+str(f-1)]-1\n",
    "        x.loc[x['fp'+str(f)]<0, 'fp'+str(f)] = 0\n",
    "        x['fp'+str(f)] = x['fp'+str(f)].astype(np.int64)\n",
    "        \n",
    "    # the failure rank of the 1st next failure for each sample\n",
    "    # for the last several samples do not have next failure, use failure_count+1\n",
    "    failure_count = sum(x['Response'])\n",
    "    x['fn1'] = x['fp1'].shift(-1).fillna(failure_count)+1\n",
    "    # the total number of failures\n",
    "    # assign np.nan to rows whose rank_failure is larger than failure_count\n",
    "    x.loc[x['fn1']>failure_count, 'fn1'] = failure_count+1\n",
    "    x['fn1'] = x['fn1'].astype(np.int64)\n",
    "\n",
    "    # the failure rank of the 2nd to failure_max (failure_max=10 here) next failure for each sample\n",
    "    for f in failure_list:\n",
    "        x['fn'+str(f)] = x['fn'+str(f-1)]+1\n",
    "        x.loc[x['fn'+str(f)]>failure_count, 'fn'+str(f)] = failure_count+1\n",
    "        x['fn'+str(f)] = x['fn'+str(f)].astype(np.int64)\n",
    "        \n",
    "    # a mapping from failure rank to start time of the failure\n",
    "    rank_failure_to_time = x.loc[x['rank_failure']!=0, ['rank_failure', 'time']].set_index(\n",
    "        'rank_failure', drop=True, inplace=False)\n",
    "    rank_failure_to_time = rank_failure_to_time.to_dict()\n",
    "    rank_failure_to_time = rank_failure_to_time['time']\n",
    "    rank_failure_to_time[0] = np.nan\n",
    "    rank_failure_to_time[failure_count+1] = np.nan\n",
    "    \n",
    "    # map from failure rank to time of the failure \n",
    "    for f in range(1, failure_max+1):\n",
    "        x['fp{}_time'.format(f)] = x['fp'+str(f)].map(rank_failure_to_time)\n",
    "        \n",
    "    # map from failure rank to time of the failure \n",
    "    for f in range(1, failure_max+1):\n",
    "        x['fn{}_time'.format(f)] = x['fn'+str(f)].map(rank_failure_to_time)\n",
    "        \n",
    "    # Calculate average of the next n failures\n",
    "    ave_list = []\n",
    "    for f in range(1, failure_max+1):\n",
    "        ave_list.append('fn{}_time'.format(f))\n",
    "        x['fn{}_time_ave{}'.format(f, suffix)] = x[ave_list].mean(axis=1) - x['time']\n",
    "\n",
    "    # Calculate average of the previous n failures\n",
    "    ave_list = []\n",
    "    for f in range(1, failure_max+1):\n",
    "        ave_list.append('fp{}_time'.format(f))\n",
    "        x['fp{}_time_ave{}'.format(f, suffix)] = x[ave_list].mean(axis=1) - x['time']\n",
    "        \n",
    "    # drop auxiliary columns\n",
    "    drop_list = [k for i in range(1, failure_max+1) for k in ['fp'+str(i), 'fn'+str(i)]]\n",
    "    drop_list.extend([k for f in range(1, failure_max+1) for k in ['fp'+str(f)+'_time', 'fn'+str(f)+'_time']])\n",
    "    drop_list.extend(['time', 'rank_sort_time', 'rank_failure', 'Response'])\n",
    "    x.drop(drop_list, axis=1, inplace=True)\n",
    "    \n",
    "    # sort index\n",
    "    x.sort_index(by='Id', axis=0, inplace=True)    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_time(time_features, by):\n",
    "    # Create time slot identifier for start time\n",
    "    u = time_features.groupby(by)[by].count()\n",
    "    u.name = 'count_'+by\n",
    "    u = pd.DataFrame(u)\n",
    "    u.reset_index(drop=False, inplace=True)\n",
    "    u.reset_index(drop=False, inplace=True)\n",
    "    u.drop(['count_'+by], axis=1, inplace=True)\n",
    "    u.columns = ['rank_'+by, by]\n",
    "    u.set_index(by, drop=True, inplace=True)\n",
    "    time_features = time_features.join(u, on=by)\n",
    "    \n",
    "    return time_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_recent(x, suffix, time_windows=['60min', '150min', '1440min', '10080min']):\n",
    "    '''\n",
    "    Count number of recent records within the given time windows both in forward and backward directions\n",
    "    '''\n",
    "    x = x.sort_values(by='time')\n",
    "    x.time = x.time/0.1\n",
    "    x = x.fillna(x.max()+2000)\n",
    "    x.time = pd.to_timedelta(x.time, unit='h')\n",
    "    x['dummy'] = 1\n",
    "\n",
    "    # Time windows \n",
    "    #     time_windows = ['60min', '150min', '1440min', '10080min']\n",
    "    # Group by start time\n",
    "    u = pd.DataFrame(x.groupby('time')['time'].count())\n",
    "    u.columns = ['counts']\n",
    "    u.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    # identifier to each time slot\n",
    "    u['rank_time'] = np.arange(len(u))\n",
    "    \n",
    "    # Backward looking time windows\n",
    "    for t in time_windows:\n",
    "        u['counts_last_{}{}'.format(t, suffix)] = u.rolling(t, on='time')['counts'].sum()\n",
    "\n",
    "    # Construct forward looking time window\n",
    "    u['time_negative'] = -u['time']\n",
    "    u.sort_values('time_negative', inplace=True)\n",
    "    for t in time_windows:\n",
    "        u['counts_next_{}{}'.format(t, suffix)] = u.rolling(t, on='time_negative')['counts'].sum()\n",
    "    u.sort_values(by='time', inplace=True)\n",
    "\n",
    "    # Drop auxiliary columns \n",
    "    u.drop(['time_negative', 'counts', 'time'], axis=1, inplace=True)\n",
    "    u.set_index('rank_time', inplace=True, drop=True)\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Featrues\n",
    "\n",
    "First do Fourier transform or autocorrelation on the time series, find the period in time. Then create the following features:\n",
    "- StartStationTimes\n",
    "    * Start time on each station\n",
    "- StartTime, EndTime, Duration\n",
    "    * Overall start and end time and duration of production\n",
    "- StationTimeDiff\n",
    "    * The time interval between two neighboring stations?\n",
    "- Start/End part of week (mod 1680)\n",
    "    * Fourier transform\n",
    "    * Autocorrelation\n",
    "- Number of records in next/last 2.5h, 24h, 168h for each station\n",
    "    * First convert numeric to `deltatime` by `x['time_start'] = pd.to_deltatime(x['time_start'], unit='h')`\n",
    "    * `df.rolling(window='60min', on='time_start')['counts'].sum()`\n",
    "- Number of records in the same time (6 mins)\n",
    "- MeanTimeDiff since last 1/5/10 failure(s)\n",
    "- MeanTimeDiff till next 1/5/10 failure(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train and test date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_train = pd.read_csv('../../data/train_date.csv.zip', index_col=0, nrows=2)\n",
    "data_types = {k: np.float16 for k in date_train.columns}\n",
    "data_types['Id'] = np.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_train = pd.read_csv('../../data/train_date.csv.zip', index_col=0, dtype=data_types)\n",
    "date_test = pd.read_csv('../../data/test_date.csv.zip', index_col=0, dtype=data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start and End Station Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = pd.MultiIndex.from_tuples([tuple(x.split('_')) for x in date_train.columns], \n",
    "                                     names=['line', 'station', 'feature'])\n",
    "date_train.columns = new_columns\n",
    "date_test.columns = new_columns\n",
    "\n",
    "date = pd.concat((date_train, date_test), keys=['train', 'test'])\n",
    "\n",
    "del date_train, date_test\n",
    "gc.collect()\n",
    "\n",
    "# sort index and column of date, so that later I can slice it\n",
    "date.sort_index(axis=0, level=[1], inplace=True, sort_remaining=True)\n",
    "date.sort_index(axis=1, level=[0, 1, 2], inplace=True)\n",
    "print(date.index.is_lexsorted(), date.columns.is_lexsorted())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start time at each station\n",
    "time_station_start = date.groupby(axis=1, level=1).min(axis=1)\n",
    "# end time at each station\n",
    "time_station_end = date.groupby(axis=1, level=1).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the time_features DataFrame\n",
    "time_features = time_station_start.join(time_station_end, lsuffix='_start', rsuffix='_end')\n",
    "\n",
    "time_station_duration = time_station_end - time_station_start\n",
    "time_station_duration.drop(time_station_duration.loc[:, time_station_duration.max(axis=0)==0].columns, \n",
    "                           axis=1, inplace=True)\n",
    "time_station_duration.columns = [k+'_duration' for k in time_station_duration.columns]\n",
    "\n",
    "# Join time_station_duration to time_features\n",
    "time_features = time_features.join(time_station_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Time, End Time, Duration\n",
    "\n",
    "Note, these features are different from the chunk/magic features, since the chunk features only include overall start time, but no end time and duration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall start, end, and duration times\n",
    "time_start = time_station_start.min(axis=1)\n",
    "time_end = time_station_end.min(axis=1)\n",
    "time_duration = time_end-time_start\n",
    "\n",
    "# Join to time_features\n",
    "time_features = time_features.join([time_start, time_end, time_duration])\n",
    "col_names = time_features.columns.tolist()\n",
    "col_names[-3:] = ['time_start', 'time_end', 'time_duration']\n",
    "time_features.columns = col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time on station transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract station flow data\n",
    "station_transition_error_rate = read_pickle('../../station_transition_error_rate_None.pickle')\n",
    "station_transition = list(station_transition_error_rate.keys())\n",
    "# Pairs of stations in all the transitions\n",
    "station_transition = [(k[0], k[1]) for k in station_transition if (k[1]!='p')&(k[1]!='n')]\n",
    "# Time used in each transition\n",
    "time_station_transition = {'S{}_to_S{}'.format(k[0], k[1]): \n",
    "                           time_station_start['S'+str(k[1])]-time_station_end['S'+str(k[0])] \n",
    "                           for k in station_transition}\n",
    "# Convert to DataFrame\n",
    "time_station_transition = pd.DataFrame(time_station_transition)\n",
    "# Join to time_features\n",
    "time_features = time_features.join(time_station_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `time_features` using `pd.to_csv` or `scipy.sparse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start/End of week (mod 1680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group production counts by the overall start time\n",
    "n_decimals = 2\n",
    "\n",
    "x = time_features['time_start']\n",
    "x = np.around(x, n_decimals)\n",
    "y = x.value_counts()\n",
    "y = y.sort_index()\n",
    "\n",
    "plt.plot(y.index, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_decimals = 2\n",
    "\n",
    "t = np.arange(y.index.min(), y.index.max(), 10**-n_decimals)\n",
    "h = np.interp(t, y.index, y.values)\n",
    "\n",
    "N = t.shape[0] # total number of time points\n",
    "T = 10**-n_decimals # time interval\n",
    "f = np.linspace(0.0, 1.0/(2.0*T), N/2) # frequency axis\n",
    "\n",
    "hf = fftpack.fft(h) # fft of h\n",
    "plt.plot(f, 10*np.log10(2.0/N*np.abs(hf[:N//2])))\n",
    "plt.axis([0.05, 0.15, 0, 30])\n",
    "plt.plot([0.0595, 0.0595], [0, 30])\n",
    "plt.plot([0.119, 0.119], [0, 30])\n",
    "plt.plot([0.05, 0.15], [20, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The production time series has frequency components as multiples of 0.0595. \n",
    "The biggest period of the time series is 1/0.06=16.8 (time unit).\n",
    "One week has 168 hours, so we can guess that 16.8 time units correspond to one week, 0.1 time unit corresponds to 1 hour, and 0.01 time unit corresponds to 6 minutes.\n",
    "\n",
    "- 16.8 is one week\n",
    "- 2.4 is one day\n",
    "- 0.1 is one hour\n",
    "- 0.01 is 6 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.correlate(h, h, 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu = u.shape[0]\n",
    "ll = int(300/T)\n",
    "\n",
    "plt.plot(t[:ll], u[lu//2:lu//2+ll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start/End part of week (mod 16.8)\n",
    "time_features['time_start_in_week'] = np.mod(time_features['time_start'], 16.8)\n",
    "time_features['time_end_in_week'] = np.mod(time_features['time_end'], 16.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of records starting or ending in the same time (6min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of records start in the same time unit\n",
    "time_features['time_start'] = np.around(time_features['time_start'], 2)\n",
    "time_features['time_end'] = np.around(time_features['time_end'], 2)\n",
    "x = time_features['time_start'].copy()\n",
    "x = pd.DataFrame(x)\n",
    "\n",
    "x1 = x.groupby('time_start')['time_start'].count()\n",
    "x1.name = 'count_time_start'\n",
    "time_features = time_features.join(x1, on='time_start', how='left')\n",
    "\n",
    "# number of records ending in the same time unit\n",
    "x = time_features['time_end'].copy()\n",
    "x = pd.DataFrame(x)\n",
    "\n",
    "x1 = x.groupby('time_end')['time_end'].count()\n",
    "x1.name = 'count_time_end'\n",
    "time_features = time_features.join(x1, on='time_end', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features.to_hdf('time_features.hdf', 'time_features', mode='w', complib='zlib', format='fixed', complevel=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of records in next/last 2.5h, 24h, 168h for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time_features\n",
    "time_features = pd.read_hdf('time_features.hdf', 'time_features')\n",
    "\n",
    "# Create identifiers of time when sorting by start or end times\n",
    "time_features = rank_time(time_features, 'time_start')\n",
    "time_features = rank_time(time_features, 'time_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of recent records within different time windows in both last and next windows (look backword and forward)\n",
    "time_windows=['60min', '150min', '240min', '480min', '720min', '1440min', '10080min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recent records when sorting by the start time\n",
    "x = time_features[['time_start', 'rank_time_start']].copy()\n",
    "x.columns = ['time', 'rank_time']\n",
    "\n",
    "u = count_recent(x, '_start', time_windows)\n",
    "# join back to time_features\n",
    "time_features = time_features.join(u, on='rank_time_start')\n",
    "\n",
    "x = time_features[['time_end', 'rank_time_end']].copy()\n",
    "x.columns = ['time', 'rank_time']\n",
    "# Count number of recent records within different time windows in both last and next windows (look backword and forward)\n",
    "u = count_recent(x, '_end', time_windows)\n",
    "# join back to time_features\n",
    "time_features = time_features.join(u, on='rank_time_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features.to_hdf('time_features.hdf', 'time_features', mode='w', complib='zlib', format='fixed', complevel=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean time difference since last/next 1/5/10 failure(s)\n",
    "\n",
    "Note: To prevent leakage from train to test, this feature should be created during CV. \n",
    "\n",
    "The procedure is:\n",
    "1. Create a DataFrame with three columns: ['Response', 'time_start', 'time_end'], the index should be Id. The Response for training data is given, but testing data have NaN in Response as if we do not know.\n",
    "2. Pass the DataFrame to the function `time_difference_to_failures`\n",
    "3. Join results to the training and testing datasets separately.\n",
    "4. Note: each iteration of CV should call this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = pd.read_hdf('time_features.hdf', 'time_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load response from training dataset\n",
    "response = pd.read_csv('../../data/train_numeric.csv.zip', index_col=0, \n",
    "                       usecols=['Id', 'Response'], dtype={'Id':np.int64, 'Response':np.int8})\n",
    "time_features = time_features.join(response, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = time_features[['Response', 'time_start', 'time_end']].copy()\n",
    "x = time_difference_to_failures(x, 10)\n",
    "\n",
    "time_features = time_features.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features.to_hdf('time_features_diff.hdf', 'time_features', mode='w', complib='zlib', format='fixed', complevel=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important numeric features are imported\n",
    "important_features = pd.read_csv('../benchmark_1/important_numeric_features.csv', index_col=0, header=None)\n",
    "important_features = list(important_features.values.ravel())\n",
    "important_features.extend(['Id', 'Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_numeric = pd.read_csv('../../data/train_numeric.csv.zip', index_col=0, usecols=important_features, dtype=np.float32)\n",
    "\n",
    "y_train = x_train_numeric.Response\n",
    "x_train_numeric.drop(['Response'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_train = pd.read_csv('../benchmark_2/train_station_flow.csv.gz', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_numeric.join(date_train)\n",
    "\n",
    "columns = list(x_train.columns)\n",
    "columns[-1] = 'station_flow'\n",
    "x_train.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_numeric = pd.read_csv('../../data/test_numeric.csv.zip', index_col=0, usecols=important_features[:-1], dtype=np.float32)\n",
    "\n",
    "date_test = pd.read_csv('../benchmark_2/test_station_flow.csv.gz', index_col=0, header=None)\n",
    "\n",
    "x_test = x_test_numeric.join(date_test)\n",
    "x_test.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.index = x_train.index.astype(np.int64)\n",
    "x_test.index = x_test.index.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train_numeric, x_test_numeric\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load benchmark_3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_chunk = pd.read_csv('../benchmark_3/start_chunk.csv.gz', index_col=0)\n",
    "\n",
    "start_chunk_train = start_chunk.loc[start_chunk.Response!=-1].drop(['Response'], axis=1)\n",
    "start_chunk_test = start_chunk.loc[start_chunk.Response==-1].drop(['Response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.join(start_chunk_train, on='Id')\n",
    "x_test = x_test.join(start_chunk_test, on='Id')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load benchmark_4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = pd.read_csv('../benchmark_4/benchmark_4_neighbors.csv.gz', index_col=0)\n",
    "\n",
    "neighbor_train = n.loc[n.Response!=-1]\n",
    "neighbor_train.drop(['Response'], axis=1, inplace=True)\n",
    "\n",
    "neighbor_test = n.loc[n.Response==-1]\n",
    "neighbor_test.drop(['Response'], axis=1, inplace=True)\n",
    "\n",
    "print(neighbor_test.shape, neighbor_train.shape)\n",
    "\n",
    "x_train = x_train.join(neighbor_train, on='Id')\n",
    "x_test = x_test.join(neighbor_test, on='Id')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add neighbor numeric features\n",
    "The important numeric features selected from previous and next records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date + station flow data will be added to x\n",
    "x = pd.concat([x_train, x_test], keys=['train', 'test'])\n",
    "x.sort_index(axis=0, level='Id', inplace=True)\n",
    "\n",
    "# Add the neighbor records\n",
    "x = x.join(x.iloc[:, :150].shift(), rsuffix='_previous')\n",
    "x = x.join(x.iloc[:, :150].shift(-1), rsuffix='_next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x.loc['train']\n",
    "x_test = x.loc['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u = x.iloc[:, :150].shift()\n",
    "# x = x.join(u, rsuffix='_previous')\n",
    "\n",
    "#pd.Series(x.columns).to_csv('x_colname.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV score based on stratified KFold with repeated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV\n",
    "# specify parameters \n",
    "# 'booster':'gbtree'\n",
    "params = {'max_depth':14, 'eta':0.03, 'silent':1, 'objective':'binary:logistic', 'nthread':12,\n",
    "         'lambda':4, 'subsample':0.9, 'min_child_weight':5, 'booster':'gbtree', 'alpha':0,\n",
    "         'base_score':0.0058, 'colsample_bytree':0.6}\n",
    "\n",
    "# 'booster':'dart'\n",
    "# params = {'max_depth':14, 'eta':0.03, 'silent':1, 'objective':'binary:logistic', 'nthread':20,\n",
    "#         'lambda':4, 'subsample':0.9, 'min_child_weight':5, 'booster':'dart', 'alpha':0,\n",
    "#         'base_score':0.0058, 'nthread':20, 'colsample_bytree':0.6, \n",
    "#         'sample_type':'uniform', 'normalize_type':'tree', 'rate_drop':0.1, 'skip_drop':0.2, 'one_drop':True}\n",
    "\n",
    "cv_results, clfs, running_time = cross_val_predict_skf_rm_xgb(params, x_train, y_train, \n",
    "                                                              num_boost_round=80, n_splits=5, \n",
    "                                                              n_repeats=3, random_state=5876967, \n",
    "                                                              verbose_eval=True)\n",
    "\n",
    "results = {'clfs': clfs, 'cv_results': cv_results, 'running_time': running_time}\n",
    "save_pickle(results, 'results_benchmark_6_cv_dart.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train_mean = cv_results['train'].mean(axis=1)\n",
    "cv_train_std = cv_results['train'].std(axis=1)\n",
    "cv_test_mean = cv_results['test'].mean(axis=1)\n",
    "cv_test_std = cv_results['test'].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(np.arange(len(cv_train_mean)), cv_train_mean)\n",
    "plt.fill_between(np.arange(len(cv_train_mean)), cv_train_mean-cv_train_std, cv_train_mean+cv_train_std, alpha=0.5)\n",
    "plt.plot(np.arange(len(cv_train_mean)), cv_test_mean)\n",
    "plt.fill_between(np.arange(len(cv_test_mean)), cv_test_mean-cv_test_std, cv_test_mean+cv_test_std, alpha=0.5)\n",
    "plt.legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "params['seed'] = 28537894\n",
    "clf = xgb.train(params, dtrain, num_boost_round=60,\n",
    "               feval=mcc_eval, evals=[(dtrain, 'train')])\n",
    "\n",
    "y_train_pred = clf.predict(dtrain)\n",
    "\n",
    "# Find best threshold \n",
    "thresholds = np.linspace(0.01, 0.99, 400)\n",
    "mcc = np.array([matthews_corrcoef(y_train, y_train_pred>thr) for thr in thresholds])\n",
    "plt.plot(thresholds, mcc)\n",
    "best_threshold = thresholds[mcc.argmax()]\n",
    "\n",
    "print('Optimal MCC = {:.3f}'.format(mcc.max()))\n",
    "print('Optimal threshold = {:.3f}'.format(best_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(x_test)\n",
    "y_test_pred = clf.predict(dtest)\n",
    "y_test_pred_int = (y_test_pred>best_threshold).astype(int)\n",
    "\n",
    "sub = pd.read_csv(\"../../data/sample_submission.csv.zip\", index_col=0)\n",
    "sub[\"Response\"] = y_test_pred_int\n",
    "sub.to_csv(\"15-benchmark_6_submission_nestimators_60_2.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_estimators=50`: Private MCC = 0.45102, public MCC = 0.43507\n",
    "\n",
    "`n_estimators=40`: CV MCC mean = 0.440, std = 0.008\n",
    "\n",
    "\n",
    "`n_estimators=75`: Private MCC = 0.41463, public MCC = 0.40368\n",
    "\n",
    "`n_estimators=60`: Private MCC = 0.45539, public MCC = 0.44090\n",
    "\n",
    "`n_estimators=60`: CV MCC mean = 0.444, std = 0.006\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
